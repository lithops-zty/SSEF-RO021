{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multivariate_model_encoder_decoder_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lithops-zty/SSEF-RO021/blob/main/multivariate_model_encoder_decoder_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T20:42:08.599823Z",
          "iopub.status.busy": "2021-09-30T20:42:08.599335Z",
          "iopub.status.idle": "2021-09-30T20:42:11.003602Z",
          "shell.execute_reply": "2021-09-30T20:42:11.002332Z",
          "shell.execute_reply.started": "2021-09-30T20:42:08.599698Z"
        },
        "id": "VeA_aFfluwcp"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "from pandas import Series\n",
        "\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, Dense,TimeDistributed, Flatten, RepeatVector\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from os.path import join\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "import keras\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "from os.path import join\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik3iNEPNRxuj"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ST1hhd8f_7X"
      },
      "source": [
        "train_year = 2019\n",
        "# test_year = 2021\n",
        "hour_bins = 3\n",
        "hour_resample = str(hour_bins) + \"H\"\n",
        "\n",
        "\n",
        "\n",
        "n_steps = 15\n",
        "n_input = 20\n",
        "n_output = 7\n",
        "rolling = False # if True, include Return and Borrow in X.\n",
        "\n",
        "train_ratio = 0.8\n",
        "valid_ratio = 0.1\n",
        "test_ratio = 1 - train_ratio - valid_ratio\n",
        "\n",
        "include_covid = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjTcNw6v7C8h"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_fmjWS3536u"
      },
      "source": [
        "train_datapath = f'/content/drive/MyDrive/Colab Notebooks/Toronto_data/data_{train_year}/{train_year}_grouped_{hour_bins}hr_multivariate.csv'\n",
        "# test_datapath = f'/content/drive/MyDrive/Colab Notebooks/Toronto_data/data_{test_year}/{test_year}_grouped_{hour_bins}hr_multivariate.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u066cgD5IZkH"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T20:42:21.222786Z",
          "iopub.status.busy": "2021-09-30T20:42:21.222402Z",
          "iopub.status.idle": "2021-09-30T20:42:22.344492Z",
          "shell.execute_reply": "2021-09-30T20:42:22.343357Z",
          "shell.execute_reply.started": "2021-09-30T20:42:21.222747Z"
        },
        "id": "G9D6SLaMJpce"
      },
      "source": [
        "def data_prep(path):\n",
        "\n",
        "  with open(path, 'r', encoding='utf8') as f:\n",
        "    dataframe = pd.read_csv(f)\n",
        "    dataframe['Time'] = pd.to_datetime(dataframe[\"Time\"], errors = \"coerce\")\n",
        "    dataframe = dataframe.set_index(['Station Id', 'Time'])\n",
        "\n",
        "\n",
        "  fields = [\"Borrow\", \"Return\", \"tempC\", \"FeelsLikeC\", \"IsHoliday\", \"Hour\", \"IsWeekday\", \"Month\", \"Latitude\", \"Longitude\"]\n",
        "  if include_covid:\n",
        "    fields.extend(['new_cases', 'new_deaths'])\n",
        "    \n",
        "  dataframe = dataframe[fields]  \n",
        "\n",
        "  dataframe['sin_time'] = np.sin(2*np.pi*dataframe.Hour/24)\n",
        "  dataframe['cos_time'] = np.cos(2*np.pi*dataframe.Hour/24)\n",
        "\n",
        "  dataframe.drop('Hour', axis=1, inplace=True)\n",
        "\n",
        "  dataframe['sin_month'] = np.sin(2*np.pi*dataframe.Month/12)\n",
        "  dataframe['cos_month'] = np.cos(2*np.pi*dataframe.Month/12)\n",
        "\n",
        "  dataframe.drop('Month', axis=1, inplace=True)\n",
        "\n",
        "  return dataframe\n",
        "\n",
        "df = data_prep(train_datapath)\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "387ANVNp1KVV"
      },
      "source": [
        "def scale_data(df):\n",
        "  columns = ['Borrow', 'Return', 'tempC', 'FeelsLikeC']\n",
        "  if include_covid:\n",
        "    columns.extend(['new_cases', 'new_deaths'])\n",
        "  df_scaled = df.copy()\n",
        "  scaling_factor = pd.Series(index=columns, dtype=float)\n",
        "  for c in scaling_factor.index:\n",
        "    negative_max = abs(df_scaled[c].min())\n",
        "    positive_max = df_scaled[c].max()\n",
        "    scaling_factor[c] = max(negative_max, positive_max)\n",
        "    if scaling_factor[c] != 0: # skip only when all values in that column == 0\n",
        "      df_scaled[c] = df_scaled[c] / scaling_factor[c]\n",
        "  return df_scaled, scaling_factor\n",
        "\n",
        "def normalise_data(df):\n",
        "  columns = ['Latitude', 'Longitude']\n",
        "  df[columns] = (df[columns]-df[columns].min())/(df[columns].max()-df[columns].min())\n",
        "  return df\n",
        "\n",
        "df_scaled, scaling_factor = scale_data(df)\n",
        "\n",
        "df_scaled = normalise_data(df_scaled)\n",
        "\n",
        "df_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4123tvvhG9nD"
      },
      "source": [
        "scaling_factor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2kgRvR3KwYY"
      },
      "source": [
        "def df23d(df): # (lvl 0 idx, lvl 1 idx, columns)\n",
        "  E = df.groupby(level=0).apply(pd.DataFrame.to_numpy)\n",
        "  return np.stack(E.values)\n",
        "\n",
        "p = array([[[2,2], [3,3], \n",
        "      [4,4], [5,5], \n",
        "      [6,6],[7,7], \n",
        "      [8,8],[9,9]]])\n",
        "\n",
        "\n",
        "\n",
        "n = 2\n",
        "def to_series(df):\n",
        "  def work(df2):\n",
        "    arr = df23d(df2)\n",
        "    arr = np.squeeze(arr)\n",
        "    return np.stack([arr[x-n_input-n_output:x] for x in range(n_input+n_output, arr.shape[0], n_steps)])\n",
        "\n",
        "  return np.concatenate(df.groupby(level=0).apply(work).values)\n",
        "    \n",
        "\n",
        "all_data = to_series(df_scaled)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SfyL7q9RExj"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_valid_data = train_test_split(all_data, train_size=train_ratio, random_state=20211207)\n",
        "validation_data, test_data = train_test_split(test_valid_data, train_size=valid_ratio/(valid_ratio+test_ratio), random_state=20211207)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGceO6wJZHbo"
      },
      "source": [
        "train_data[:,:20,:2].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BqmcwCuTXRY"
      },
      "source": [
        "def split_xy(data):\n",
        "  return (data[:, :n_input] if rolling else data[:, :n_input, 2:]), data[:, n_input:, :2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD7NTEI-pva4"
      },
      "source": [
        "\n",
        "train_X, train_Y = split_xy(train_data)\n",
        "val_X, val_Y = split_xy(validation_data)\n",
        "test_X, test_Y = split_xy(test_data)\n",
        "\n",
        "train_X.shape, train_Y.shape, val_X.shape, val_Y.shape, test_X.shape, test_Y.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev4Ghnf18pdO"
      },
      "source": [
        "test_Y[459]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIKRJz6bIhhd"
      },
      "source": [
        "# Build & Train & Test Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = f'{train_year}_{hour_bins}hr_'\n",
        "if rolling:\n",
        "  model_name += 'rolling_'\n",
        "if include_covid:\n",
        "  model_name += 'include_covid_'\n",
        "model_name += 'best_model'"
      ],
      "metadata": {
        "id": "BYUFB9PrMULQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8GaKbnzJiZX"
      },
      "source": [
        "def init_model():\n",
        "  n_in_features, n_out_features = train_X.shape[2], train_Y.shape[2]\n",
        "\t\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(00, activation='relu', input_shape=(n_input, n_in_features), dropout = 0.2))\n",
        "  model.add(RepeatVector(n_output))\n",
        "  model.add(LSTM(00, activation='relu', return_sequences=True, dropout = 0.2))\n",
        "  model.add(TimeDistributed(Dense(50, activation='relu')))\n",
        "  model.add(TimeDistributed(Dense(n_out_features, activation=\"relu\")))\n",
        "  model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "  model.summary()\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "def fit_model(model):\n",
        "  epochs, batch_size = 25, 128\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "\n",
        "  mc = ModelCheckpoint(f'/content/drive/MyDrive/Colab Notebooks/Toronto_data/models/{model_name}', monitor='val_loss')\n",
        "\n",
        "  history = model.fit(train_X, train_Y, \n",
        "                      epochs=epochs, \n",
        "                      batch_size=batch_size, \n",
        "                      validation_data=(val_X, val_Y), \n",
        "                      callbacks = [mc,es])\n",
        "\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muIr0fKkXIHu"
      },
      "source": [
        "model = init_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZmQ9YZ8B4N_"
      },
      "source": [
        "# model = tf.keras.models.load_model(f'/content/drive/MyDrive/Colab Notebooks/Toronto_data/models/{model_name}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hW6fDaagaPs"
      },
      "source": [
        "history = fit_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd_UMsQTbWci"
      },
      "source": [
        "# save_path = f'/content/drive/MyDrive/Colab Notebooks/Toronto_data/models/{train_year}_{hour_bins}hr_{\"rolling_\" if rolling else \"\"}best_model'\n",
        "# model.save(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ACOm6fDDEk4"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "plt.savefig(\"Model_loss_dropout.png\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FujhOUPwmdpg"
      },
      "source": [
        "model.evaluate(test_X, test_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYSp8kC4EjO6"
      },
      "source": [
        "def predict(stn_id, pred_start):\n",
        "  pred_start = pd.Timestamp(pred_start)\n",
        "  X_end = pred_start - pd.Timedelta(hours=1 * hour_bins)\n",
        "  X_start = X_end - pd.Timedelta(hours=(n_input-1) * hour_bins)\n",
        "\n",
        "  X = df_scaled.loc[(stn_id, X_start):(stn_id, X_end)]\n",
        "  arr_X = X.iloc[:,(0 if rolling else 2):].to_numpy()[None,:,:]\n",
        "\n",
        "  pred_interval = pd.date_range(pred_start, pred_start + pd.Timedelta(hours=(n_output-1) * hour_bins), freq=hour_resample)\n",
        "  pred = model.predict(arr_X)[0]\n",
        "  pred[:,0] = pred[:,0] * scaling_factor['Borrow']\n",
        "  pred[:,1] = pred[:,1] * scaling_factor['Return']\n",
        "\n",
        "  return pd.DataFrame({'Borrow':pred[:,0], 'Return':pred[:,1]},index=pred_interval)\n",
        "\n",
        "predict(7000, f'{train_year}/2/1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O95I3iqHYaKF"
      },
      "source": [
        "def truth(stn_id, start_time, end_time):      \n",
        "  start_time = pd.Timestamp(start_time)\n",
        "  end_time = pd.Timestamp(end_time)\n",
        "  return df[['Borrow', 'Return']].loc[stn_id].loc[start_time:end_time]\n",
        "\n",
        "truth(7019, f'{train_year}/1/2', f'{train_year}/1/3 00:00:00')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e54dGgYX0vfW"
      },
      "source": [
        "from random import randrange, choice\n",
        "def plot_pred(stn_id, start_time):\n",
        "  fig, (ax0, ax1) = plt.subplots(1,2, figsize=(18,5))\n",
        "\n",
        "  df_X = truth(stn_id, pd.Timestamp(start_time)-pd.Timedelta(hours=n_input*hour_bins), start_time - pd.Timedelta(hours=1*hour_bins))\n",
        "  df_truth = pd.concat([df_X.iloc[-1:], truth(stn_id, start_time, start_time + pd.Timedelta(hours=(n_output-1)*hour_bins))], axis=0)\n",
        "  df_pred = pd.concat([df_X.iloc[-1:], predict(stn_id, start_time)], axis=0).round()\n",
        "\n",
        "  df_truth['Borrow'].plot(ax=ax0, label='truth')\n",
        "  df_pred['Borrow'].plot(ax=ax0, label='predicted')\n",
        "  df_X['Borrow'].plot(ax=ax0, label='input')\n",
        "  \n",
        "  df_truth['Return'].plot(ax=ax1, label='truth')\n",
        "  df_pred['Return'].plot(ax=ax1, label='predicted')\n",
        "  df_X['Return'].plot(ax=ax1, label='input')\n",
        "\n",
        "  ax0.set_title(f'Bike Borrowed: Station ID={stn_id}')\n",
        "  ax1.set_title(f'Bike Returned: Station ID={stn_id}')\n",
        "  ax0.legend()\n",
        "  ax1.legend()\n",
        "\n",
        "\n",
        "\n",
        "num = randrange(7004, 7500)\n",
        "time = choice(pd.date_range(f'{train_year}/2/3', f'{train_year}/12/10', freq=hour_resample))\n",
        "\n",
        "\n",
        "plot_pred(7075, time)\n",
        "plt.savefig('Model_prediction_dropout.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyz8ZoQV6dfE"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "034zlOEm8Ax3"
      },
      "source": [
        "Toronto went into first stage of lockdown from 23 March 2020 to 23 June 2020.\n",
        "\n",
        "Toronto went into Stage 2 from 24 June 2020 to 30 July 2020.\n",
        "\n",
        "Toronto went into Stage 3 from 31 July 2020.\n",
        "\n",
        "Toronto was placed under lockdown on 23 November 2020.\n",
        "\n",
        "Toronto had a province-wide shutdown from 26 December 2020.\n",
        "\n",
        "Second state of emergency in January 2021 and stay-at-home orders.\n",
        "\n",
        "Stay-at-home orders lifted on March 8, 2021. \n",
        "\n",
        "April 3, 2021, entered a shutdown coupled with a stay-at-home order that lasted until June 2, 2021"
      ]
    }
  ]
}